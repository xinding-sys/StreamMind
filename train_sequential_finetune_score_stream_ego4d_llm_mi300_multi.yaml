description: finetune_videollama2_mamba_batch1_stream_epoch15_ego4d_only_trainllm_1223_8

target:
  service: sing
  name: msroctobasicvc
  workspace_name: srgxws

environment:
  image: amlt-sing/acpt-rocm6.1_ubuntu20.04_py3.9_pytorch2.1.2 
  setup:
    - pip install azureml-mlflow tensorboard --user
    - sudo apt-get update
    - sudo apt-get install -y htop ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6
    - pip install --no-cache-dir --upgrade pip wheel

storage:
  input:
    storage_account_name: sanbpx4p3idss6q   # TODO!!
    container_name: v-dingxin  # TODO!!
    mount_dir: /mnt/input/

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: /home/v-dingxin/videollama2_plus-main   #TODO!!

jobs:
  - name: finetune_videollama2_mamba_batch1_stream_epoch15_ego4d_only_trainllm_1223_8
    sku: 8x192G8-MI300X-IB-xGMI
    mpi: True
    process_count_per_node: 1
    command:
    - export RCCL_MSCCL_ENABLE=0
    - export NCCL_TOPO_FILE=gcr
    - export PYTORCH_HIP_ALLOC_CONF=expandable_segments:True
    - pip install decord
    - pip install -r require_yizhao.txt
    - pip install transformers==4.44.2
    - pip install timm
    - pip install lightning
    - pip install accelerate
    - pip install scikit-image
    - pip install moviepy==1.0.3 --user
    - pip install scenedetect==0.6.3
    - pip install wandb==0.18.3
    - pip install ftfy regex Pillow opencv-python timm==0.6.12 einops fvcore
    - git clone https://github.com/state-spaces/mamba.git
    - cd mamba 
    - pip install . --no-build-isolation
    - cd ..
    - git clone https://github.com/Dao-AILab/flash-attention.git
    - cd flash-attention
    - git checkout 418d677192b483dfc1decfdf9aadca40b402485d  # v2.6.3
    - BUILD_TARGET=rocm python setup.py install
    - cd ..
    - pip install tensorboard -U
    - torchrun --nproc_per_node=8 --nnodes=8 --node_rank=$$OMPI_COMM_WORLD_RANK --master_addr="$$MASTER_ADDR" --master_port=$$MASTER_PORT videollama2/train_flash_attn_score.py
        --output_dir /mnt/input/finetune_videollama2_mamba_batch1_stream_epoch15_ego4d_only_trainllm_1223_8
        --version v1_mistral
        --ego4d_dataset True
        --soccer_dataset_train_llm True 
        --vision_tower /mnt/input/clip-vit-large-patch14-336
        --freeze_backbone False
        --mm_projector_type mamba
        --model_name_or_path /mnt/input/VideoLLaMA2-7B
        --data_path  /mnt/input/video_llm/datasets/videollava_pt/valley_llavaimage_new.json
        --data_folder /mnt/input/video_llm/datasets/videollava_pt
        --mm_vision_select_layer -2
        --mm_use_im_start_end False
        --mm_use_im_patch_token False
        --image_aspect_ratio pad
        --num_frames 32
        --bf16 True
        --tf32 False
        --fp16 False
        --num_train_epochs 15
        --per_device_train_batch_size 1
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 2
        --evaluation_strategy "no"
        --save_strategy "steps"
        --save_steps 5000
        --save_total_limit 99
        --learning_rate 2e-5
        --weight_decay 0.
        --warmup_ratio 0.03
        --lr_scheduler_type "cosine"
        --logging_steps 1
        --model_max_length 20480
        --gradient_checkpointing True
        --dataloader_num_workers 8
        --report_to wandb
        --run_name finetune_videollama2_mamba_batch1_stream_epoch15_ego4d_only_trainllm_1223_8
    identity: managed
    submit_args:
      env:
        WANDB_API_KEY: 'c2814c78978395e25bb69a6d987a233e8b0f076f'
        _AZUREML_SINGULARITY_JOB_UAI: /subscriptions/e546c811-2312-46e2-9fd0-c949c65da4d9/resourcegroups/srgx/providers/Microsoft.ManagedIdentity/userAssignedIdentities/srgxuai
    sla_tier: Standard
    execution_mode: basic 
    priority: high